<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><title>Linear Regression: Theory and Implementation - Arjun Chidambaram Subbiah</title><meta name="author" content="Arjun Subbiah"/><meta name="description" content="A comprehensive guide to linear regression, from mathematical foundations to practical implementation in Python."/><meta property="og:title" content="Linear Regression: Theory and Implementation - Arjun Chidambaram Subbiah"/><meta property="og:description" content="A comprehensive guide to linear regression, from mathematical foundations to practical implementation in Python."/><meta property="og:image" content="undefined/arjun-subbiah-og.png"/><meta property="og:image:alt" content="Arjun Chidambaram Subbiah&#x27;s Blog"/><meta property="og:url" content="undefined/blog/posts/linear-regression-theory-implementation"/><meta property="og:site_name" content="Arjun Chidambaram Subbiah&#x27;s Blog"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:image:alt" content="Arjun Chidambaram Subbiah&#x27;s Blog"/><meta property="og:type" content="article"/><meta name="next-head-count" content="14"/><meta charSet="utf-8"/><link rel="apple-touch-icon" sizes="180x180" href="/favicons/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicons/favicon-16x16.png"/><link rel="shortcut icon" href="/favicons/favicon.ico"/><link rel="manifest" href="/favicons/site.webmanifest"/><link rel="mask-icon" href="/favicons/safari-pinned-tab.svg" color="#5bbad5"/><meta name="msapplication-TileColor" content="#5bbad5"/><meta name="theme-color" content="#1d2a35"/><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin /><link rel="preload" href="/_next/static/css/72c1cbe3c149fe6e.css" as="style" crossorigin=""/><link rel="stylesheet" href="/_next/static/css/72c1cbe3c149fe6e.css" crossorigin="" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" crossorigin="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-2555a4296ab7a1b2.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/framework-0c7baedefba6b077.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/main-c998894f89caec06.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/pages/_app-802e58e1e19c9d3c.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/664-0356555cfd50fb3d.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/675-f5ea2ef57a5d96b0.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/135-7114c347a895ccce.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/pages/blog/posts/%5Bslug%5D-903e210a46044a16.js" defer="" crossorigin=""></script><script src="/_next/static/xTgmcbjv4Cbvb4ngwGNIT/_buildManifest.js" defer="" crossorigin=""></script><script src="/_next/static/xTgmcbjv4Cbvb4ngwGNIT/_ssgManifest.js" defer="" crossorigin=""></script><style data-href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,400;0,500;0,600;0,700;1,400&display=swap">@font-face{font-family:'Jost';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/jost/v19/92zJtBhPNqw73oHH7BbQp4-B6XlrZu0FNI4.woff) format('woff')}@font-face{font-family:'Jost';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/jost/v19/92zPtBhPNqw79Ij1E865zBUv7myjJQVF.woff) format('woff')}@font-face{font-family:'Jost';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/jost/v19/92zPtBhPNqw79Ij1E865zBUv7myRJQVF.woff) format('woff')}@font-face{font-family:'Jost';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/jost/v19/92zPtBhPNqw79Ij1E865zBUv7mx9IgVF.woff) format('woff')}@font-face{font-family:'Jost';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/jost/v19/92zPtBhPNqw79Ij1E865zBUv7mxEIgVF.woff) format('woff')}@font-face{font-family:'Jost';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/jost/v19/92zJtBhPNqw73oHH7BbQp4-B6XlrZu0FBI4kmNPOIEtWC4Z3.woff) format('woff');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Jost';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/jost/v19/92zJtBhPNqw73oHH7BbQp4-B6XlrZu0FBIQkmNPOIEtWC4Z3.woff) format('woff');unicode-range:U+0100-02BA,U+02BD-02C5,U+02C7-02CC,U+02CE-02D7,U+02DD-02FF,U+0304,U+0308,U+0329,U+1D00-1DBF,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20C0,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Jost';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/jost/v19/92zJtBhPNqw73oHH7BbQp4-B6XlrZu0FBIokmNPOIEtWCw.woff) format('woff');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Jost';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/jost/v19/92zatBhPNqw73oDd4jQmfxIC7w.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Jost';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/jost/v19/92zatBhPNqw73ord4jQmfxIC7w.woff2) format('woff2');unicode-range:U+0100-02BA,U+02BD-02C5,U+02C7-02CC,U+02CE-02D7,U+02DD-02FF,U+0304,U+0308,U+0329,U+1D00-1DBF,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20C0,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Jost';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/jost/v19/92zatBhPNqw73oTd4jQmfxI.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Jost';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/jost/v19/92zatBhPNqw73oDd4jQmfxIC7w.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Jost';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/jost/v19/92zatBhPNqw73ord4jQmfxIC7w.woff2) format('woff2');unicode-range:U+0100-02BA,U+02BD-02C5,U+02C7-02CC,U+02CE-02D7,U+02DD-02FF,U+0304,U+0308,U+0329,U+1D00-1DBF,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20C0,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Jost';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/jost/v19/92zatBhPNqw73oTd4jQmfxI.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Jost';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/jost/v19/92zatBhPNqw73oDd4jQmfxIC7w.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Jost';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/jost/v19/92zatBhPNqw73ord4jQmfxIC7w.woff2) format('woff2');unicode-range:U+0100-02BA,U+02BD-02C5,U+02C7-02CC,U+02CE-02D7,U+02DD-02FF,U+0304,U+0308,U+0329,U+1D00-1DBF,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20C0,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Jost';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/jost/v19/92zatBhPNqw73oTd4jQmfxI.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Jost';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/jost/v19/92zatBhPNqw73oDd4jQmfxIC7w.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Jost';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/jost/v19/92zatBhPNqw73ord4jQmfxIC7w.woff2) format('woff2');unicode-range:U+0100-02BA,U+02BD-02C5,U+02C7-02CC,U+02CE-02D7,U+02DD-02FF,U+0304,U+0308,U+0329,U+1D00-1DBF,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20C0,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Jost';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/jost/v19/92zatBhPNqw73oTd4jQmfxI.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}</style></head><body class="bg-bglight dark:bg-bgdark"><div id="__next"><script>!function(){try{var d=document.documentElement,c=d.classList;c.remove('light','dark');var e=localStorage.getItem('theme');if('system'===e||(!e&&true)){var t='(prefers-color-scheme: dark)',m=window.matchMedia(t);if(m.media!==t||m.matches){d.style.colorScheme = 'dark';c.add('dark')}else{d.style.colorScheme = 'light';c.add('light')}}else if(e){c.add(e|| '')}if(e==='light'||e==='dark')d.style.colorScheme=e}catch(e){}}()</script><div id="fb-root"></div><div class="bg-bglight dark:bg-bgdark"><div class="selection:bg-marrsgreen selection:text-bglight dark:selection:bg-carrigreen dark:selection:text-bgdark"><a role="button" class="py-2 px-3 absolute left-2 opacity-95 outline-marrsgreen  dark:outline-carrigreen rounded-b-lg transition-transform -translate-y-52 focus:transform focus:translate-y-0 lg:text-xl z-50 bg-marrsgreen dark:bg-carrigreen text-textlight dark:text-bgdark" href="#main">Skip to main content</a><header class="md:flex"><div class="lower-glassmorphism bg-bglight dark:bg-bgdark z-30 top-0 shadow-sm fixed duration-400 px-4 sm:px-8 h-16 w-full "><div class="w-full h-full mx-auto max-w-6xl flex items-center justify-between"><a class="after:content-[&#x27;blog&#x27;] after:bg-bgdark dark:after:bg-bglight after:text-textlight dark:after:text-bgdark after:text-base after:px-2 after:inline-block after:rotate-12 after:absolute after:-right-12 hover:after:rotate-0 relative text-xl sm:text-2xl hover:text-marrsgreen dark:hover:text-carrigreen focus-visible:outline-marrsgreen dark:focus-visible:outline-carrigreen" href="/blog">arjunchidambaram<span class="text-marrsgreen dark:text-carrigreen">.subbiah</span></a><nav class="flex items-center"><div class="glassmorphism md:bg-transparent md:dark:bg-transparent md:backdrop-blur-none fixed md:static bottom-4 z-30 left-1/2 md:left-auto transform -translate-x-1/2 md:transform-none bg-bglight dark:bg-carddark dark:text-textlight w-11/12 rounded drop-shadow-lg md:drop-shadow-none"><ul class="flex justify-evenly items-center py-1"><li class="md:hidden"><a class="text-sm md:text-lg flex flex-col items-center w-[4.5rem] md:w-auto md:mr-6  dark:fill-textlight md:hover:text-marrsgreen md:dark:hover:text-carrigreen link-outline
                        false" href="/blog"><span class="md:hidden"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" class=""><path d="M12.71 2.29a1 1 0 0 0-1.42 0l-9 9a1 1 0 0 0 0 1.42A1 1 0 0 0 3 13h1v7a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2v-7h1a1 1 0 0 0 1-1 1 1 0 0 0-.29-.71zM6 20v-9.59l6-6 6 6V20z"></path></svg></span><span class="whitespace-nowrap">Home</span></a></li><li class=""><a class="text-sm md:text-lg flex flex-col items-center w-[4.5rem] md:w-auto md:mr-6  dark:fill-textlight md:hover:text-marrsgreen md:dark:hover:text-carrigreen link-outline
                        false" href="/blog/categories"><span class="md:hidden"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" class=""><path d="M10 3H4a1 1 0 0 0-1 1v6a1 1 0 0 0 1 1h6a1 1 0 0 0 1-1V4a1 1 0 0 0-1-1zM9 9H5V5h4v4zm11-6h-6a1 1 0 0 0-1 1v6a1 1 0 0 0 1 1h6a1 1 0 0 0 1-1V4a1 1 0 0 0-1-1zm-1 6h-4V5h4v4zm-9 4H4a1 1 0 0 0-1 1v6a1 1 0 0 0 1 1h6a1 1 0 0 0 1-1v-6a1 1 0 0 0-1-1zm-1 6H5v-4h4v4zm8-6c-2.206 0-4 1.794-4 4s1.794 4 4 4 4-1.794 4-4-1.794-4-4-4zm0 6c-1.103 0-2-.897-2-2s.897-2 2-2 2 .897 2 2-.897 2-2 2z"></path></svg></span><span class="whitespace-nowrap">Categories</span></a></li><li class=""><a class="text-sm md:text-lg flex flex-col items-center w-[4.5rem] md:w-auto md:mr-6  dark:fill-textlight md:hover:text-marrsgreen md:dark:hover:text-carrigreen link-outline
                        false" href="/blog/tags"><span class="md:hidden"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" class=""><path d="M20 4H8.515a2 2 0 0 0-1.627.838l-4.701 6.581a.997.997 0 0 0 0 1.162l4.701 6.581A2 2 0 0 0 8.515 20H20c1.103 0 2-.897 2-2V6c0-1.103-.897-2-2-2zm0 14H8.515l-4.286-6 4.286-6H20v12z"></path></svg></span><span class="whitespace-nowrap">Tags</span></a></li><li class=""><a class="text-sm md:text-lg flex flex-col items-center w-[4.5rem] md:w-auto md:mr-6  dark:fill-textlight md:hover:text-marrsgreen md:dark:hover:text-carrigreen link-outline
                        false" href="/"><span class="md:hidden"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" class=""><path d="M12 2a5 5 0 1 0 5 5 5 5 0 0 0-5-5zm0 8a3 3 0 1 1 3-3 3 3 0 0 1-3 3zm9 11v-1a7 7 0 0 0-7-7h-4a7 7 0 0 0-7 7v1h2v-1a5 5 0 0 1 5-5h4a5 5 0 0 1 5 5v1z"></path></svg></span><span class="whitespace-nowrap">Portfolio</span></a></li></ul></div><button type="button" title="Toggles light &amp; dark theme" aria-live="polite" class="w-8 h-8 rounded-lg flex justify-center items-center link-outline"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" class="fill-textlight hidden dark:inline-block transform scale-110 md:dark:hover:fill-carrigreen"><path d="M6.993 12c0 2.761 2.246 5.007 5.007 5.007s5.007-2.246 5.007-5.007S14.761 6.993 12 6.993 6.993 9.239 6.993 12zM12 8.993c1.658 0 3.007 1.349 3.007 3.007S13.658 15.007 12 15.007 8.993 13.658 8.993 12 10.342 8.993 12 8.993zM10.998 19h2v3h-2zm0-17h2v3h-2zm-9 9h3v2h-3zm17 0h3v2h-3zM4.219 18.363l2.12-2.122 1.415 1.414-2.12 2.122zM16.24 6.344l2.122-2.122 1.414 1.414-2.122 2.122zM6.342 7.759 4.22 5.637l1.415-1.414 2.12 2.122zm13.434 10.605-1.414 1.414-2.122-2.122 1.414-1.414z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" class="dark:hidden transform scale-90 md:hover:fill-marrsgreen "><path d="M20.742 13.045a8.088 8.088 0 0 1-2.077.271c-2.135 0-4.14-.83-5.646-2.336a8.025 8.025 0 0 1-2.064-7.723A1 1 0 0 0 9.73 2.034a10.014 10.014 0 0 0-4.489 2.582c-3.898 3.898-3.898 10.243 0 14.143a9.937 9.937 0 0 0 7.072 2.93 9.93 9.93 0 0 0 7.07-2.929 10.007 10.007 0 0 0 2.583-4.491 1.001 1.001 0 0 0-1.224-1.224zm-2.772 4.301a7.947 7.947 0 0 1-5.656 2.343 7.953 7.953 0 0 1-5.658-2.344c-3.118-3.119-3.118-8.195 0-11.314a7.923 7.923 0 0 1 2.06-1.483 10.027 10.027 0 0 0 2.89 7.848 9.972 9.972 0 0 0 7.848 2.891 8.036 8.036 0 0 1-1.484 2.059z"></path></svg></button></nav></div></div></header><div class="hidden fixed left-10 bottom-0 md:flex flex-col w-6 h-56 items-center justify-between"><div class="-rotate-90 text-lg tracking-widest"><a href="mailto:undefined" class="link-outline hover:text-marrsgreen dark:hover:text-carrigreen"></a></div><div class="w-40 h-1 bg-bgdark dark:bg-bglight rotate-90"></div></div><div class="hidden fixed right-10 bottom-0 md:flex flex-col w-6 h-[17rem] items-center justify-between"><div class="flex flex-col space-y-6"><a title="Arjun Subbiah&#x27;s Github Profile" href="https://github.com/arjunsubbiah" class="scale-110 rounded link-outline"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" class="dark:fill-bglight hover:fill-marrsgreen dark:hover:fill-carrigreen"><path fill-rule="evenodd" clip-rule="evenodd" d="M12.026 2c-5.509 0-9.974 4.465-9.974 9.974 0 4.406 2.857 8.145 6.821 9.465.499.09.679-.217.679-.481 0-.237-.008-.865-.011-1.696-2.775.602-3.361-1.338-3.361-1.338-.452-1.152-1.107-1.459-1.107-1.459-.905-.619.069-.605.069-.605 1.002.07 1.527 1.028 1.527 1.028.89 1.524 2.336 1.084 2.902.829.091-.645.351-1.085.635-1.334-2.214-.251-4.542-1.107-4.542-4.93 0-1.087.389-1.979 1.024-2.675-.101-.253-.446-1.268.099-2.64 0 0 .837-.269 2.742 1.021a9.582 9.582 0 0 1 2.496-.336 9.554 9.554 0 0 1 2.496.336c1.906-1.291 2.742-1.021 2.742-1.021.545 1.372.203 2.387.099 2.64.64.696 1.024 1.587 1.024 2.675 0 3.833-2.33 4.675-4.552 4.922.355.308.675.916.675 1.846 0 1.334-.012 2.41-.012 2.737 0 .267.178.577.687.479C19.146 20.115 22 16.379 22 11.974 22 6.465 17.535 2 12.026 2z"></path></svg></a><a title="Arjun Subbiah&#x27;s LinkedIn Profile" href="https://www.linkedin.com/in/arjun-subbiah-b19330a6/" class="scale-110 rounded link-outline"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" class="dark:fill-bglight hover:fill-marrsgreen dark:hover:fill-carrigreen"><path d="M20 3H4a1 1 0 0 0-1 1v16a1 1 0 0 0 1 1h16a1 1 0 0 0 1-1V4a1 1 0 0 0-1-1zM8.339 18.337H5.667v-8.59h2.672v8.59zM7.003 8.574a1.548 1.548 0 1 1 0-3.096 1.548 1.548 0 0 1 0 3.096zm11.335 9.763h-2.669V14.16c0-.996-.018-2.277-1.388-2.277-1.39 0-1.601 1.086-1.601 2.207v4.248h-2.667v-8.59h2.56v1.174h.037c.355-.675 1.227-1.387 2.524-1.387 2.704 0 3.203 1.778 3.203 4.092v4.71z"></path></svg></a><a title="Check Arjun Subbiah on Dev.to" href="https://dev.to/arjunsubbiah" class="scale-110 rounded link-outline"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" class="dark:fill-bglight hover:fill-marrsgreen dark:hover:fill-carrigreen"><path d="M7.826 10.083a.784.784 0 0 0-.468-.175h-.701v4.198h.701a.786.786 0 0 0 .469-.175c.155-.117.233-.292.233-.525v-2.798c.001-.233-.079-.408-.234-.525zM19.236 3H4.764C3.791 3 3.002 3.787 3 4.76v14.48c.002.973.791 1.76 1.764 1.76h14.473c.973 0 1.762-.787 1.764-1.76V4.76A1.765 1.765 0 0 0 19.236 3zM9.195 13.414c0 .755-.466 1.901-1.942 1.898H5.389V8.665h1.903c1.424 0 1.902 1.144 1.903 1.899v2.85zm4.045-3.562H11.1v1.544h1.309v1.188H11.1v1.543h2.142v1.188h-2.498a.813.813 0 0 1-.833-.792V9.497a.813.813 0 0 1 .792-.832h2.539l-.002 1.187zm4.165 4.632c-.531 1.235-1.481.99-1.906 0l-1.548-5.818h1.309l1.193 4.569 1.188-4.569h1.31l-1.546 5.818z"></path></svg></a><a title="Arjun Subbiah&#x27;s Profile on Facebook" href="https://www.facebook.com/arjunsubbiah" class="scale-110 rounded link-outline"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" class="dark:fill-bglight hover:fill-marrsgreen dark:hover:fill-carrigreen"><path d="M12.001 2.002c-5.522 0-9.999 4.477-9.999 9.999 0 4.99 3.656 9.126 8.437 9.879v-6.988h-2.54v-2.891h2.54V9.798c0-2.508 1.493-3.891 3.776-3.891 1.094 0 2.24.195 2.24.195v2.459h-1.264c-1.24 0-1.628.772-1.628 1.563v1.875h2.771l-.443 2.891h-2.328v6.988C18.344 21.129 22 16.992 22 12.001c0-5.522-4.477-9.999-9.999-9.999z"></path></svg></a></div><div class="w-40 h-1 bg-bgdark dark:bg-bglight rotate-90"></div></div><main id="main" class="blog-main"><article class="blog-section"><h1 class="font-semibold md:font-bold text-3xl md:text-4xl">Linear Regression: Theory and Implementation</h1><div class="mt-2 mb-1 italic text-marrsdark dark:text-carrigreen"><div class="relative"><span class="sr-only">Posted on: </span> <span aria-hidden="true">|</span><span class="sr-only"> at </span> </div></div><span class="flex items-center mt-2 mb-2 px-2 border-l-4 border-marrsgreen dark:border-carrigreen"><svg xmlns="http://www.w3.org/2000/svg" class="h-6 w-6 inline-block mr-2" fill="none" viewBox="0 0 24 24" aria-label="Category" stroke="currentColor" stroke-width="1"><path stroke-linecap="round" stroke-linejoin="round" d="M5 19a2 2 0 01-2-2V7a2 2 0 012-2h4l2 2h4a2 2 0 012 2v1M5 19h14a2 2 0 002-2v-5a2 2 0 00-2-2H9a2 2 0 00-2 2v5a2 2 0 01-2 2z"></path></svg> <a class="hover:text-marrsgreen dark:hover:text-carrigreen font-medium outline-marrsgreen dark:outline-carrigreen" href="/blog/categories/machine-learning">Machine Learning</a></span><div class="my-2"><a class="blog-tag link-outline group whitespace-nowrap dark:fill-bglight hover:bg-marrsgreen hover:text-bglight hover:fill-bglight dark:hover:bg-carrigreen dark:hover:text-bgdark dark:hover:fill-bgdark py-1 px-2 text-xs mr-2 my-1 bg-gray-300 dark:bg-carddark rounded inline-block shadow hover:shadow-md cursor-pointer" href="/blog/tags/machine-learning"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" class="inline-block scale-75 mr-1"><path d="M13.707 3.293A.996.996 0 0 0 13 3H4a1 1 0 0 0-1 1v9c0 .266.105.52.293.707l8 8a.997.997 0 0 0 1.414 0l9-9a.999.999 0 0 0 0-1.414l-8-8zM12 19.586l-7-7V5h7.586l7 7L12 19.586z"></path><circle cx="8.496" cy="8.495" r="1.505"></circle></svg>machine-learning</a><a class="blog-tag link-outline group whitespace-nowrap dark:fill-bglight hover:bg-marrsgreen hover:text-bglight hover:fill-bglight dark:hover:bg-carrigreen dark:hover:text-bgdark dark:hover:fill-bgdark py-1 px-2 text-xs mr-2 my-1 bg-gray-300 dark:bg-carddark rounded inline-block shadow hover:shadow-md cursor-pointer" href="/blog/tags/statistics"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" class="inline-block scale-75 mr-1"><path d="M13.707 3.293A.996.996 0 0 0 13 3H4a1 1 0 0 0-1 1v9c0 .266.105.52.293.707l8 8a.997.997 0 0 0 1.414 0l9-9a.999.999 0 0 0 0-1.414l-8-8zM12 19.586l-7-7V5h7.586l7 7L12 19.586z"></path><circle cx="8.496" cy="8.495" r="1.505"></circle></svg>statistics</a><a class="blog-tag link-outline group whitespace-nowrap dark:fill-bglight hover:bg-marrsgreen hover:text-bglight hover:fill-bglight dark:hover:bg-carrigreen dark:hover:text-bgdark dark:hover:fill-bgdark py-1 px-2 text-xs mr-2 my-1 bg-gray-300 dark:bg-carddark rounded inline-block shadow hover:shadow-md cursor-pointer" href="/blog/tags/python"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" class="inline-block scale-75 mr-1"><path d="M13.707 3.293A.996.996 0 0 0 13 3H4a1 1 0 0 0-1 1v9c0 .266.105.52.293.707l8 8a.997.997 0 0 0 1.414 0l9-9a.999.999 0 0 0 0-1.414l-8-8zM12 19.586l-7-7V5h7.586l7 7L12 19.586z"></path><circle cx="8.496" cy="8.495" r="1.505"></circle></svg>python</a><a class="blog-tag link-outline group whitespace-nowrap dark:fill-bglight hover:bg-marrsgreen hover:text-bglight hover:fill-bglight dark:hover:bg-carrigreen dark:hover:text-bgdark dark:hover:fill-bgdark py-1 px-2 text-xs mr-2 my-1 bg-gray-300 dark:bg-carddark rounded inline-block shadow hover:shadow-md cursor-pointer" href="/blog/tags/regression"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" class="inline-block scale-75 mr-1"><path d="M13.707 3.293A.996.996 0 0 0 13 3H4a1 1 0 0 0-1 1v9c0 .266.105.52.293.707l8 8a.997.997 0 0 0 1.414 0l9-9a.999.999 0 0 0 0-1.414l-8-8zM12 19.586l-7-7V5h7.586l7 7L12 19.586z"></path><circle cx="8.496" cy="8.495" r="1.505"></circle></svg>regression</a></div><div class="prose prose-img:mx-auto prose-lg prose-headings:font-medium prose-h2:mb-3 prose-h3:mb-1 prose-h3:mt-6 prose-h3:text-xl prose-h3:font-bold prose-h3:italic prose-h3:text-marrsgreen dark:prose-h3:text-carrigreen prose-p:mt-1 prose-p:my-3 mx-auto max-w-4xl dark:prose-invert"><div><h1>Linear Regression: Theory and Implementation</h1>
<p>Linear regression is the foundation of machine learning. As a Staff Data Scientist who has implemented countless ML models in production, I can tell you that mastering linear regression is crucial for any data scientist.</p>
<h2>What is Linear Regression?</h2>
<p>Linear regression models the relationship between a dependent variable and independent variables by fitting a linear equation to observed data.</p>
<h2>Mathematical Foundation</h2>
<p>The basic linear regression equation is:</p>
<p>$$y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \dots + \beta_nx_n + \epsilon$$</p>
<p>Where:</p>
<ul>
<li>$y$ is the dependent variable (target)</li>
<li>$\beta_0$ is the intercept term</li>
<li>$\beta_1, \beta_2, \dots, \beta_n$ are the coefficients</li>
<li>$x_1, x_2, \dots, x_n$ are the independent variables (features)</li>
<li>$\epsilon$ is the error term</li>
</ul>
<h2>The Optimization Problem</h2>
<p>Linear regression finds coefficients by minimizing the Mean Squared Error (MSE):</p>
<p>$$MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$</p>
<p>This is solved using the Normal Equation or Gradient Descent.</p>
<h2>Implementation in Python</h2>
<pre><code>import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

# Generate sample data
np.random.seed(42)
n_samples = 100
X = np.random.randn(n_samples, 3)  # 3 features
true_coefficients = [2.5, -1.2, 3.8]
y = X @ true_coefficients + np.random.randn(n_samples) * 0.5

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions
y_pred_train = model.predict(X_train)
y_pred_test = model.predict(X_test)

# Evaluate model
train_mse = mean_squared_error(y_train, y_pred_train)
test_mse = mean_squared_error(y_test, y_pred_test)
train_r2 = r2_score(y_train, y_pred_train)
test_r2 = r2_score(y_test, y_pred_test)

print("Model Performance:")
print(f"Training MSE: {train_mse:.4f}")
print(f"Testing MSE: {test_mse:.4f}")
print(f"Training R²: {train_r2:.4f}")
print(f"Testing R²: {test_r2:.4f}")

print(f"\nLearned Coefficients: {model.coef_}")
print(f"True Coefficients: {true_coefficients}")
print(f"Intercept: {model.intercept_:.4f}")
</code></pre>
<h2>Key Assumptions of Linear Regression</h2>
<p>Understanding these assumptions is crucial for successful implementation:</p>
<h3>1. Linearity</h3>
<p>The relationship between features and target must be linear. Check with scatter plots and residual analysis.</p>
<h3>2. Independence</h3>
<p>Observations must be independent of each other. Violating this leads to unreliable standard errors.</p>
<h3>3. Homoscedasticity</h3>
<p>Constant variance of residuals. Plot residuals vs. fitted values to check.</p>
<h3>4. Normality of Residuals</h3>
<p>Residuals should be normally distributed. Use Q-Q plots and statistical tests.</p>
<h2>Diagnostic Tools</h2>
<pre><code>import scipy.stats as stats

# Residual analysis
residuals = y_test - y_pred_test

# 1. Linearity check - residuals vs fitted
plt.figure(figsize=(12, 4))

plt.subplot(1, 3, 1)
plt.scatter(y_pred_test, residuals)
plt.axhline(y=0, color='red', linestyle='--')
plt.xlabel('Fitted Values')
plt.ylabel('Residuals')
plt.title('Residuals vs Fitted')

# 2. Normality check - Q-Q plot
plt.subplot(1, 3, 2)
stats.probplot(residuals, dist="norm", plot=plt)
plt.title('Q-Q Plot')

# 3. Homoscedasticity check
plt.subplot(1, 3, 3)
plt.scatter(y_pred_test, np.abs(residuals))
plt.xlabel('Fitted Values')
plt.ylabel('|Residuals|')
plt.title('Scale-Location Plot')

plt.tight_layout()
plt.show()

# Statistical tests
from scipy.stats import jarque_bera, shapiro

# Normality tests
jb_stat, jb_pvalue = jarque_bera(residuals)
shapiro_stat, shapiro_pvalue = shapiro(residuals)

print(f"Jarque-Bera test p-value: {jb_pvalue:.4f}")
print(f"Shapiro-Wilk test p-value: {shapiro_pvalue:.4f}")
</code></pre>
<h2>Real-World Applications</h2>
<p>In my experience at Walmart, I've used linear regression for:</p>
<h3>1. Demand Forecasting</h3>
<pre><code># Example: Predicting product demand
features = ['historical_sales', 'seasonality_factor', 'price', 'promotion_flag', 'competitor_price']
# Simple, interpretable model for stakeholder buy-in
demand_model = LinearRegression()
</code></pre>
<h3>2. Cost Optimization</h3>
<p>Linear regression helped optimize delivery costs by modeling the relationship between:</p>
<ul>
<li>Distance, weight, and delivery time</li>
<li>Driver allocation and operational costs</li>
<li>Route efficiency factors</li>
</ul>
<h3>3. Performance Monitoring</h3>
<p>Linear baselines for A/B testing and performance monitoring:</p>
<ul>
<li>Simple to implement and explain</li>
<li>Fast training and prediction</li>
<li>Reliable baseline for comparing complex models</li>
</ul>
<h2>When to Use Linear Regression</h2>
<p><strong>Use Linear Regression When:</strong></p>
<ul>
<li>You need interpretable results</li>
<li>Relationships are approximately linear</li>
<li>You want a fast, simple baseline</li>
<li>Sample size is small to medium</li>
<li>Stakeholders need to understand model decisions</li>
</ul>
<p><strong>Avoid When:</strong></p>
<ul>
<li>Relationships are highly non-linear</li>
<li>You have categorical variables with many levels</li>
<li>Multicollinearity is severe</li>
<li>You need maximum predictive accuracy over interpretability</li>
</ul>
<h2>Advanced Techniques</h2>
<h3>Regularization</h3>
<p>Handle overfitting and multicollinearity:</p>
<pre><code>from sklearn.linear_model import Ridge, Lasso, ElasticNet

# Ridge regression (L2 regularization)
ridge = Ridge(alpha=1.0)
ridge.fit(X_train, y_train)

# Lasso regression (L1 regularization)
lasso = Lasso(alpha=0.1)
lasso.fit(X_train, y_train)

# Elastic Net (combines L1 and L2)
elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)
elastic_net.fit(X_train, y_train)
</code></pre>
<h3>Feature Engineering</h3>
<p>Enhance linear regression with:</p>
<ul>
<li>Polynomial features: <code>PolynomialFeatures()</code></li>
<li>Interaction terms: Multiply features</li>
<li>Log transformations: For exponential relationships</li>
<li>Standardization: For fair coefficient comparison</li>
</ul>
<h2>Production Lessons Learned</h2>
<p>From deploying linear regression models in production:</p>
<h3>1. Always Validate Assumptions</h3>
<p>Use automated checks in your ML pipeline:</p>
<pre><code>def validate_linear_regression_assumptions(X, y, model):
    """Automated assumption checking"""
    predictions = model.predict(X)
    residuals = y - predictions
    
    # Check linearity, homoscedasticity, normality
    # Return warnings if assumptions are violated
    return validation_report
</code></pre>
<h3>2. Monitor Model Drift</h3>
<p>Linear regression can degrade gracefully:</p>
<ul>
<li>Coefficients remain stable in good models</li>
<li>Monitor R² and MSE over time</li>
<li>Set up alerts for significant changes</li>
</ul>
<h3>3. Feature Engineering Is Key</h3>
<p>The "linear" in linear regression refers to coefficients, not relationships:</p>
<ul>
<li>Create polynomial features for curves</li>
<li>Use log transforms for exponential relationships</li>
<li>Engineer interaction terms for complex relationships</li>
</ul>
<h3>4. Communicate Uncertainty</h3>
<p>Always provide confidence intervals:</p>
<pre><code>from scipy import stats

# Calculate prediction intervals
def prediction_intervals(model, X, confidence=0.95):
    predictions = model.predict(X)
    # Calculate standard errors and intervals
    # Return lower and upper bounds
    return lower_bounds, upper_bounds
</code></pre>
<h2>Conclusion</h2>
<p>Linear regression remains one of my most-used algorithms because of its simplicity, interpretability, and solid performance on many real-world problems. While newer algorithms might achieve higher accuracy, linear regression provides:</p>
<ul>
<li>Transparent decision-making</li>
<li>Fast training and prediction</li>
<li>Reliable baseline performance</li>
<li>Easy debugging and monitoring</li>
</ul>
<p>Master linear regression first, then build toward more complex algorithms. The intuition you develop here will serve you throughout your machine learning journey.</p>
<p>As I've learned from deploying models that drive millions of dollars in business value: sometimes the simplest approach is the most powerful.</p>
<p>What's your experience with linear regression? Have you encountered interesting applications or challenges? I'd love to hear your thoughts in the comments below.</p>
</div></div><div class="fb-share-button my-4" data-href="undefined/blog/posts/linear-regression-theory-implementation" data-layout="button" data-size="large"><a rel="noopener noreferrer" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=undefined%2Fblog%2Fposts%2Flinear-regression-theory-implementation&amp;amp;src=sdkpreparse" class="fb-xfbml-parse-ignore">Share</a></div></article></main><footer class="pb-24 md:pb-4 text-center mt-auto"><div class="flex justify-center space-x-12 mb-4"><a href="https://github.com/arjunsubbiah" title="Arjun Chidambaram Subbiah&#x27;s Github Profile" class="transform scale-150 md:scale-125 link-outline"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" class="dark:fill-bglight hover:fill-marrsgreen dark:hover:fill-carrigreen"><path fill-rule="evenodd" clip-rule="evenodd" d="M12.026 2c-5.509 0-9.974 4.465-9.974 9.974 0 4.406 2.857 8.145 6.821 9.465.499.09.679-.217.679-.481 0-.237-.008-.865-.011-1.696-2.775.602-3.361-1.338-3.361-1.338-.452-1.152-1.107-1.459-1.107-1.459-.905-.619.069-.605.069-.605 1.002.07 1.527 1.028 1.527 1.028.89 1.524 2.336 1.084 2.902.829.091-.645.351-1.085.635-1.334-2.214-.251-4.542-1.107-4.542-4.93 0-1.087.389-1.979 1.024-2.675-.101-.253-.446-1.268.099-2.64 0 0 .837-.269 2.742 1.021a9.582 9.582 0 0 1 2.496-.336 9.554 9.554 0 0 1 2.496.336c1.906-1.291 2.742-1.021 2.742-1.021.545 1.372.203 2.387.099 2.64.64.696 1.024 1.587 1.024 2.675 0 3.833-2.33 4.675-4.552 4.922.355.308.675.916.675 1.846 0 1.334-.012 2.41-.012 2.737 0 .267.178.577.687.479C19.146 20.115 22 16.379 22 11.974 22 6.465 17.535 2 12.026 2z"></path></svg></a><a href="https://linkedin.com/in/arjun-subbiah-b19330a6/" title="Arjun Chidambaram Subbiah&#x27;s LinkedIn Profile" class="transform scale-150 md:scale-125 link-outline"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" class="dark:fill-bglight hover:fill-marrsgreen dark:hover:fill-carrigreen"><path d="M20 3H4a1 1 0 0 0-1 1v16a1 1 0 0 0 1 1h16a1 1 0 0 0 1-1V4a1 1 0 0 0-1-1zM8.339 18.337H5.667v-8.59h2.672v8.59zM7.003 8.574a1.548 1.548 0 1 1 0-3.096 1.548 1.548 0 0 1 0 3.096zm11.335 9.763h-2.669V14.16c0-.996-.018-2.277-1.388-2.277-1.39 0-1.601 1.086-1.601 2.207v4.248h-2.667v-8.59h2.56v1.174h.037c.355-.675 1.227-1.387 2.524-1.387 2.704 0 3.203 1.778 3.203 4.092v4.71z"></path></svg></a><a href="https://twitter.com/arjunsubbiah" title="Follow Arjun Chidambaram Subbiah on Twitter" class="transform scale-150 md:scale-125 link-outline"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" class="dark:fill-bglight hover:fill-marrsgreen dark:hover:fill-carrigreen"><path d="M19.633 7.997c.013.175.013.349.013.523 0 5.325-4.053 11.461-11.46 11.461-2.282 0-4.402-.661-6.186-1.809.324.037.636.05.973.05a8.07 8.07 0 0 0 5.001-1.721 4.036 4.036 0 0 1-3.767-2.793c.249.037.499.062.761.062.361 0 .724-.05 1.061-.137a4.027 4.027 0 0 1-3.23-3.953v-.05c.537.299 1.16.486 1.82.511a4.022 4.022 0 0 1-1.796-3.354c0-.748.199-1.434.548-2.032a11.457 11.457 0 0 0 8.306 4.215c-.062-.3-.1-.611-.1-.923a4.026 4.026 0 0 1 4.028-4.028c1.16 0 2.207.486 2.943 1.272a7.957 7.957 0 0 0 2.556-.973 4.02 4.02 0 0 1-1.771 2.22 8.073 8.073 0 0 0 2.319-.624 8.645 8.645 0 0 1-2.019 2.083z"></path></svg></a></div><div>Coded with <span class="sr-only">love</span><svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mx-1 inline-block mb-1" fill="none" viewBox="0 0 24 24" stroke="currentColor" aria-hidden="true" stroke-width="2"><path stroke-linecap="round" stroke-linejoin="round" d="M4.318 6.318a4.5 4.5 0 000 6.364L12 20.364l7.682-7.682a4.5 4.5 0 00-6.364-6.364L12 7.636l-1.318-1.318a4.5 4.5 0 00-6.364 0z"></path></svg> <!-- -->by <!-- -->Arjun Chidambaram Subbiah</div></footer></div></div></div><script id="__NEXT_DATA__" type="application/json" crossorigin="">{"props":{"pageProps":{"post":{"title":"Linear Regression: Theory and Implementation","datetime":"2024-07-20T10:35:07.000Z","description":"A comprehensive guide to linear regression, from mathematical foundations to practical implementation in Python.","slug":"linear-regression-theory-implementation","author":"Arjun Subbiah","content":"\u003ch1\u003eLinear Regression: Theory and Implementation\u003c/h1\u003e\n\u003cp\u003eLinear regression is the foundation of machine learning. As a Staff Data Scientist who has implemented countless ML models in production, I can tell you that mastering linear regression is crucial for any data scientist.\u003c/p\u003e\n\u003ch2\u003eWhat is Linear Regression?\u003c/h2\u003e\n\u003cp\u003eLinear regression models the relationship between a dependent variable and independent variables by fitting a linear equation to observed data.\u003c/p\u003e\n\u003ch2\u003eMathematical Foundation\u003c/h2\u003e\n\u003cp\u003eThe basic linear regression equation is:\u003c/p\u003e\n\u003cp\u003e$$y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\dots + \\beta_nx_n + \\epsilon$$\u003c/p\u003e\n\u003cp\u003eWhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$y$ is the dependent variable (target)\u003c/li\u003e\n\u003cli\u003e$\\beta_0$ is the intercept term\u003c/li\u003e\n\u003cli\u003e$\\beta_1, \\beta_2, \\dots, \\beta_n$ are the coefficients\u003c/li\u003e\n\u003cli\u003e$x_1, x_2, \\dots, x_n$ are the independent variables (features)\u003c/li\u003e\n\u003cli\u003e$\\epsilon$ is the error term\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eThe Optimization Problem\u003c/h2\u003e\n\u003cp\u003eLinear regression finds coefficients by minimizing the Mean Squared Error (MSE):\u003c/p\u003e\n\u003cp\u003e$$MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\u003c/p\u003e\n\u003cp\u003eThis is solved using the Normal Equation or Gradient Descent.\u003c/p\u003e\n\u003ch2\u003eImplementation in Python\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003eimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\n\n# Generate sample data\nnp.random.seed(42)\nn_samples = 100\nX = np.random.randn(n_samples, 3)  # 3 features\ntrue_coefficients = [2.5, -1.2, 3.8]\ny = X @ true_coefficients + np.random.randn(n_samples) * 0.5\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Make predictions\ny_pred_train = model.predict(X_train)\ny_pred_test = model.predict(X_test)\n\n# Evaluate model\ntrain_mse = mean_squared_error(y_train, y_pred_train)\ntest_mse = mean_squared_error(y_test, y_pred_test)\ntrain_r2 = r2_score(y_train, y_pred_train)\ntest_r2 = r2_score(y_test, y_pred_test)\n\nprint(\"Model Performance:\")\nprint(f\"Training MSE: {train_mse:.4f}\")\nprint(f\"Testing MSE: {test_mse:.4f}\")\nprint(f\"Training R²: {train_r2:.4f}\")\nprint(f\"Testing R²: {test_r2:.4f}\")\n\nprint(f\"\\nLearned Coefficients: {model.coef_}\")\nprint(f\"True Coefficients: {true_coefficients}\")\nprint(f\"Intercept: {model.intercept_:.4f}\")\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eKey Assumptions of Linear Regression\u003c/h2\u003e\n\u003cp\u003eUnderstanding these assumptions is crucial for successful implementation:\u003c/p\u003e\n\u003ch3\u003e1. Linearity\u003c/h3\u003e\n\u003cp\u003eThe relationship between features and target must be linear. Check with scatter plots and residual analysis.\u003c/p\u003e\n\u003ch3\u003e2. Independence\u003c/h3\u003e\n\u003cp\u003eObservations must be independent of each other. Violating this leads to unreliable standard errors.\u003c/p\u003e\n\u003ch3\u003e3. Homoscedasticity\u003c/h3\u003e\n\u003cp\u003eConstant variance of residuals. Plot residuals vs. fitted values to check.\u003c/p\u003e\n\u003ch3\u003e4. Normality of Residuals\u003c/h3\u003e\n\u003cp\u003eResiduals should be normally distributed. Use Q-Q plots and statistical tests.\u003c/p\u003e\n\u003ch2\u003eDiagnostic Tools\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003eimport scipy.stats as stats\n\n# Residual analysis\nresiduals = y_test - y_pred_test\n\n# 1. Linearity check - residuals vs fitted\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 3, 1)\nplt.scatter(y_pred_test, residuals)\nplt.axhline(y=0, color='red', linestyle='--')\nplt.xlabel('Fitted Values')\nplt.ylabel('Residuals')\nplt.title('Residuals vs Fitted')\n\n# 2. Normality check - Q-Q plot\nplt.subplot(1, 3, 2)\nstats.probplot(residuals, dist=\"norm\", plot=plt)\nplt.title('Q-Q Plot')\n\n# 3. Homoscedasticity check\nplt.subplot(1, 3, 3)\nplt.scatter(y_pred_test, np.abs(residuals))\nplt.xlabel('Fitted Values')\nplt.ylabel('|Residuals|')\nplt.title('Scale-Location Plot')\n\nplt.tight_layout()\nplt.show()\n\n# Statistical tests\nfrom scipy.stats import jarque_bera, shapiro\n\n# Normality tests\njb_stat, jb_pvalue = jarque_bera(residuals)\nshapiro_stat, shapiro_pvalue = shapiro(residuals)\n\nprint(f\"Jarque-Bera test p-value: {jb_pvalue:.4f}\")\nprint(f\"Shapiro-Wilk test p-value: {shapiro_pvalue:.4f}\")\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eReal-World Applications\u003c/h2\u003e\n\u003cp\u003eIn my experience at Walmart, I've used linear regression for:\u003c/p\u003e\n\u003ch3\u003e1. Demand Forecasting\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003e# Example: Predicting product demand\nfeatures = ['historical_sales', 'seasonality_factor', 'price', 'promotion_flag', 'competitor_price']\n# Simple, interpretable model for stakeholder buy-in\ndemand_model = LinearRegression()\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e2. Cost Optimization\u003c/h3\u003e\n\u003cp\u003eLinear regression helped optimize delivery costs by modeling the relationship between:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDistance, weight, and delivery time\u003c/li\u003e\n\u003cli\u003eDriver allocation and operational costs\u003c/li\u003e\n\u003cli\u003eRoute efficiency factors\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e3. Performance Monitoring\u003c/h3\u003e\n\u003cp\u003eLinear baselines for A/B testing and performance monitoring:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSimple to implement and explain\u003c/li\u003e\n\u003cli\u003eFast training and prediction\u003c/li\u003e\n\u003cli\u003eReliable baseline for comparing complex models\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eWhen to Use Linear Regression\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eUse Linear Regression When:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eYou need interpretable results\u003c/li\u003e\n\u003cli\u003eRelationships are approximately linear\u003c/li\u003e\n\u003cli\u003eYou want a fast, simple baseline\u003c/li\u003e\n\u003cli\u003eSample size is small to medium\u003c/li\u003e\n\u003cli\u003eStakeholders need to understand model decisions\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eAvoid When:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eRelationships are highly non-linear\u003c/li\u003e\n\u003cli\u003eYou have categorical variables with many levels\u003c/li\u003e\n\u003cli\u003eMulticollinearity is severe\u003c/li\u003e\n\u003cli\u003eYou need maximum predictive accuracy over interpretability\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eAdvanced Techniques\u003c/h2\u003e\n\u003ch3\u003eRegularization\u003c/h3\u003e\n\u003cp\u003eHandle overfitting and multicollinearity:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom sklearn.linear_model import Ridge, Lasso, ElasticNet\n\n# Ridge regression (L2 regularization)\nridge = Ridge(alpha=1.0)\nridge.fit(X_train, y_train)\n\n# Lasso regression (L1 regularization)\nlasso = Lasso(alpha=0.1)\nlasso.fit(X_train, y_train)\n\n# Elastic Net (combines L1 and L2)\nelastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)\nelastic_net.fit(X_train, y_train)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eFeature Engineering\u003c/h3\u003e\n\u003cp\u003eEnhance linear regression with:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePolynomial features: \u003ccode\u003ePolynomialFeatures()\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eInteraction terms: Multiply features\u003c/li\u003e\n\u003cli\u003eLog transformations: For exponential relationships\u003c/li\u003e\n\u003cli\u003eStandardization: For fair coefficient comparison\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eProduction Lessons Learned\u003c/h2\u003e\n\u003cp\u003eFrom deploying linear regression models in production:\u003c/p\u003e\n\u003ch3\u003e1. Always Validate Assumptions\u003c/h3\u003e\n\u003cp\u003eUse automated checks in your ML pipeline:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edef validate_linear_regression_assumptions(X, y, model):\n    \"\"\"Automated assumption checking\"\"\"\n    predictions = model.predict(X)\n    residuals = y - predictions\n    \n    # Check linearity, homoscedasticity, normality\n    # Return warnings if assumptions are violated\n    return validation_report\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e2. Monitor Model Drift\u003c/h3\u003e\n\u003cp\u003eLinear regression can degrade gracefully:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCoefficients remain stable in good models\u003c/li\u003e\n\u003cli\u003eMonitor R² and MSE over time\u003c/li\u003e\n\u003cli\u003eSet up alerts for significant changes\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e3. Feature Engineering Is Key\u003c/h3\u003e\n\u003cp\u003eThe \"linear\" in linear regression refers to coefficients, not relationships:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCreate polynomial features for curves\u003c/li\u003e\n\u003cli\u003eUse log transforms for exponential relationships\u003c/li\u003e\n\u003cli\u003eEngineer interaction terms for complex relationships\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e4. Communicate Uncertainty\u003c/h3\u003e\n\u003cp\u003eAlways provide confidence intervals:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom scipy import stats\n\n# Calculate prediction intervals\ndef prediction_intervals(model, X, confidence=0.95):\n    predictions = model.predict(X)\n    # Calculate standard errors and intervals\n    # Return lower and upper bounds\n    return lower_bounds, upper_bounds\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eConclusion\u003c/h2\u003e\n\u003cp\u003eLinear regression remains one of my most-used algorithms because of its simplicity, interpretability, and solid performance on many real-world problems. While newer algorithms might achieve higher accuracy, linear regression provides:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTransparent decision-making\u003c/li\u003e\n\u003cli\u003eFast training and prediction\u003c/li\u003e\n\u003cli\u003eReliable baseline performance\u003c/li\u003e\n\u003cli\u003eEasy debugging and monitoring\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eMaster linear regression first, then build toward more complex algorithms. The intuition you develop here will serve you throughout your machine learning journey.\u003c/p\u003e\n\u003cp\u003eAs I've learned from deploying models that drive millions of dollars in business value: sometimes the simplest approach is the most powerful.\u003c/p\u003e\n\u003cp\u003eWhat's your experience with linear regression? Have you encountered interesting applications or challenges? I'd love to hear your thoughts in the comments below.\u003c/p\u003e\n","category":"Machine Learning","tags":["machine-learning","statistics","python","regression"]}},"__N_SSG":true},"page":"/blog/posts/[slug]","query":{"slug":"linear-regression-theory-implementation"},"buildId":"xTgmcbjv4Cbvb4ngwGNIT","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>