<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><title>Decision Trees: From Basics to Random Forests - Arjun Chidambaram Subbiah</title><meta name="author" content="Arjun Subbiah"/><meta name="description" content="Master decision trees and understand how they evolve into powerful ensemble methods like Random Forests."/><meta property="og:title" content="Decision Trees: From Basics to Random Forests - Arjun Chidambaram Subbiah"/><meta property="og:description" content="Master decision trees and understand how they evolve into powerful ensemble methods like Random Forests."/><meta property="og:image" content="undefined/arjun-subbiah-og.png"/><meta property="og:image:alt" content="Arjun Chidambaram Subbiah&#x27;s Blog"/><meta property="og:url" content="undefined/blog/posts/decision-trees-random-forests"/><meta property="og:site_name" content="Arjun Chidambaram Subbiah&#x27;s Blog"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:image:alt" content="Arjun Chidambaram Subbiah&#x27;s Blog"/><meta property="og:type" content="article"/><meta name="next-head-count" content="14"/><meta charSet="utf-8"/><link rel="apple-touch-icon" sizes="180x180" href="/favicons/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicons/favicon-16x16.png"/><link rel="shortcut icon" href="/favicons/favicon.ico"/><link rel="manifest" href="/favicons/site.webmanifest"/><link rel="mask-icon" href="/favicons/safari-pinned-tab.svg" color="#5bbad5"/><meta name="msapplication-TileColor" content="#5bbad5"/><meta name="theme-color" content="#1d2a35"/><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin /><link rel="preload" href="/_next/static/css/72c1cbe3c149fe6e.css" as="style" crossorigin=""/><link rel="stylesheet" href="/_next/static/css/72c1cbe3c149fe6e.css" crossorigin="" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" crossorigin="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-2555a4296ab7a1b2.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/framework-0c7baedefba6b077.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/main-c998894f89caec06.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/pages/_app-802e58e1e19c9d3c.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/664-0356555cfd50fb3d.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/675-f5ea2ef57a5d96b0.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/135-7114c347a895ccce.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/pages/blog/posts/%5Bslug%5D-903e210a46044a16.js" defer="" crossorigin=""></script><script src="/_next/static/Oo8n1X-_SUSxiqmo-5vbM/_buildManifest.js" defer="" crossorigin=""></script><script src="/_next/static/Oo8n1X-_SUSxiqmo-5vbM/_ssgManifest.js" defer="" crossorigin=""></script><style data-href="https://fonts.googleapis.com/css2?family=Jost:ital,wght@0,400;0,500;0,600;0,700;1,400&display=swap">@font-face{font-family:'Jost';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/jost/v19/92zJtBhPNqw73oHH7BbQp4-B6XlrZu0FNI4.woff) format('woff')}@font-face{font-family:'Jost';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/jost/v19/92zPtBhPNqw79Ij1E865zBUv7myjJQVF.woff) format('woff')}@font-face{font-family:'Jost';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/jost/v19/92zPtBhPNqw79Ij1E865zBUv7myRJQVF.woff) format('woff')}@font-face{font-family:'Jost';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/jost/v19/92zPtBhPNqw79Ij1E865zBUv7mx9IgVF.woff) format('woff')}@font-face{font-family:'Jost';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/jost/v19/92zPtBhPNqw79Ij1E865zBUv7mxEIgVF.woff) format('woff')}@font-face{font-family:'Jost';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/jost/v19/92zJtBhPNqw73oHH7BbQp4-B6XlrZu0FBI4kmNPOIEtWC4Z3.woff) format('woff');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Jost';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/jost/v19/92zJtBhPNqw73oHH7BbQp4-B6XlrZu0FBIQkmNPOIEtWC4Z3.woff) format('woff');unicode-range:U+0100-02BA,U+02BD-02C5,U+02C7-02CC,U+02CE-02D7,U+02DD-02FF,U+0304,U+0308,U+0329,U+1D00-1DBF,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20C0,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Jost';font-style:italic;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/jost/v19/92zJtBhPNqw73oHH7BbQp4-B6XlrZu0FBIokmNPOIEtWCw.woff) format('woff');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Jost';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/jost/v19/92zatBhPNqw73oDd4jQmfxIC7w.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Jost';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/jost/v19/92zatBhPNqw73ord4jQmfxIC7w.woff2) format('woff2');unicode-range:U+0100-02BA,U+02BD-02C5,U+02C7-02CC,U+02CE-02D7,U+02DD-02FF,U+0304,U+0308,U+0329,U+1D00-1DBF,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20C0,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Jost';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/jost/v19/92zatBhPNqw73oTd4jQmfxI.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Jost';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/jost/v19/92zatBhPNqw73oDd4jQmfxIC7w.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Jost';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/jost/v19/92zatBhPNqw73ord4jQmfxIC7w.woff2) format('woff2');unicode-range:U+0100-02BA,U+02BD-02C5,U+02C7-02CC,U+02CE-02D7,U+02DD-02FF,U+0304,U+0308,U+0329,U+1D00-1DBF,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20C0,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Jost';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/jost/v19/92zatBhPNqw73oTd4jQmfxI.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Jost';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/jost/v19/92zatBhPNqw73oDd4jQmfxIC7w.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Jost';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/jost/v19/92zatBhPNqw73ord4jQmfxIC7w.woff2) format('woff2');unicode-range:U+0100-02BA,U+02BD-02C5,U+02C7-02CC,U+02CE-02D7,U+02DD-02FF,U+0304,U+0308,U+0329,U+1D00-1DBF,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20C0,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Jost';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/jost/v19/92zatBhPNqw73oTd4jQmfxI.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Jost';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/jost/v19/92zatBhPNqw73oDd4jQmfxIC7w.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Jost';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/jost/v19/92zatBhPNqw73ord4jQmfxIC7w.woff2) format('woff2');unicode-range:U+0100-02BA,U+02BD-02C5,U+02C7-02CC,U+02CE-02D7,U+02DD-02FF,U+0304,U+0308,U+0329,U+1D00-1DBF,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20C0,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Jost';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/jost/v19/92zatBhPNqw73oTd4jQmfxI.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}</style></head><body class="bg-bglight dark:bg-bgdark"><div id="__next"><script>!function(){try{var d=document.documentElement,c=d.classList;c.remove('light','dark');var e=localStorage.getItem('theme');if('system'===e||(!e&&true)){var t='(prefers-color-scheme: dark)',m=window.matchMedia(t);if(m.media!==t||m.matches){d.style.colorScheme = 'dark';c.add('dark')}else{d.style.colorScheme = 'light';c.add('light')}}else if(e){c.add(e|| '')}if(e==='light'||e==='dark')d.style.colorScheme=e}catch(e){}}()</script><div id="fb-root"></div><div class="bg-bglight dark:bg-bgdark"><div class="selection:bg-marrsgreen selection:text-bglight dark:selection:bg-carrigreen dark:selection:text-bgdark"><a role="button" class="py-2 px-3 absolute left-2 opacity-95 outline-marrsgreen  dark:outline-carrigreen rounded-b-lg transition-transform -translate-y-52 focus:transform focus:translate-y-0 lg:text-xl z-50 bg-marrsgreen dark:bg-carrigreen text-textlight dark:text-bgdark" href="#main">Skip to main content</a><header class="md:flex"><div class="lower-glassmorphism bg-bglight dark:bg-bgdark z-30 top-0 shadow-sm fixed duration-400 px-4 sm:px-8 h-16 w-full "><div class="w-full h-full mx-auto max-w-6xl flex items-center justify-between"><a class="after:content-[&#x27;blog&#x27;] after:bg-bgdark dark:after:bg-bglight after:text-textlight dark:after:text-bgdark after:text-base after:px-2 after:inline-block after:rotate-12 after:absolute after:-right-12 hover:after:rotate-0 relative text-xl sm:text-2xl hover:text-marrsgreen dark:hover:text-carrigreen focus-visible:outline-marrsgreen dark:focus-visible:outline-carrigreen" href="/blog">arjunchidambaram<span class="text-marrsgreen dark:text-carrigreen">.subbiah</span></a><nav class="flex items-center"><div class="glassmorphism md:bg-transparent md:dark:bg-transparent md:backdrop-blur-none fixed md:static bottom-4 z-30 left-1/2 md:left-auto transform -translate-x-1/2 md:transform-none bg-bglight dark:bg-carddark dark:text-textlight w-11/12 rounded drop-shadow-lg md:drop-shadow-none"><ul class="flex justify-evenly items-center py-1"><li class="md:hidden"><a class="text-sm md:text-lg flex flex-col items-center w-[4.5rem] md:w-auto md:mr-6  dark:fill-textlight md:hover:text-marrsgreen md:dark:hover:text-carrigreen link-outline
                        false" href="/blog"><span class="md:hidden"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" class=""><path d="M12.71 2.29a1 1 0 0 0-1.42 0l-9 9a1 1 0 0 0 0 1.42A1 1 0 0 0 3 13h1v7a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2v-7h1a1 1 0 0 0 1-1 1 1 0 0 0-.29-.71zM6 20v-9.59l6-6 6 6V20z"></path></svg></span><span class="whitespace-nowrap">Home</span></a></li><li class=""><a class="text-sm md:text-lg flex flex-col items-center w-[4.5rem] md:w-auto md:mr-6  dark:fill-textlight md:hover:text-marrsgreen md:dark:hover:text-carrigreen link-outline
                        false" href="/blog/categories"><span class="md:hidden"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" class=""><path d="M10 3H4a1 1 0 0 0-1 1v6a1 1 0 0 0 1 1h6a1 1 0 0 0 1-1V4a1 1 0 0 0-1-1zM9 9H5V5h4v4zm11-6h-6a1 1 0 0 0-1 1v6a1 1 0 0 0 1 1h6a1 1 0 0 0 1-1V4a1 1 0 0 0-1-1zm-1 6h-4V5h4v4zm-9 4H4a1 1 0 0 0-1 1v6a1 1 0 0 0 1 1h6a1 1 0 0 0 1-1v-6a1 1 0 0 0-1-1zm-1 6H5v-4h4v4zm8-6c-2.206 0-4 1.794-4 4s1.794 4 4 4 4-1.794 4-4-1.794-4-4-4zm0 6c-1.103 0-2-.897-2-2s.897-2 2-2 2 .897 2 2-.897 2-2 2z"></path></svg></span><span class="whitespace-nowrap">Categories</span></a></li><li class=""><a class="text-sm md:text-lg flex flex-col items-center w-[4.5rem] md:w-auto md:mr-6  dark:fill-textlight md:hover:text-marrsgreen md:dark:hover:text-carrigreen link-outline
                        false" href="/blog/tags"><span class="md:hidden"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" class=""><path d="M20 4H8.515a2 2 0 0 0-1.627.838l-4.701 6.581a.997.997 0 0 0 0 1.162l4.701 6.581A2 2 0 0 0 8.515 20H20c1.103 0 2-.897 2-2V6c0-1.103-.897-2-2-2zm0 14H8.515l-4.286-6 4.286-6H20v12z"></path></svg></span><span class="whitespace-nowrap">Tags</span></a></li><li class=""><a class="text-sm md:text-lg flex flex-col items-center w-[4.5rem] md:w-auto md:mr-6  dark:fill-textlight md:hover:text-marrsgreen md:dark:hover:text-carrigreen link-outline
                        false" href="/"><span class="md:hidden"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" class=""><path d="M12 2a5 5 0 1 0 5 5 5 5 0 0 0-5-5zm0 8a3 3 0 1 1 3-3 3 3 0 0 1-3 3zm9 11v-1a7 7 0 0 0-7-7h-4a7 7 0 0 0-7 7v1h2v-1a5 5 0 0 1 5-5h4a5 5 0 0 1 5 5v1z"></path></svg></span><span class="whitespace-nowrap">Portfolio</span></a></li></ul></div><button type="button" title="Toggles light &amp; dark theme" aria-live="polite" class="w-8 h-8 rounded-lg flex justify-center items-center link-outline"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" class="fill-textlight hidden dark:inline-block transform scale-110 md:dark:hover:fill-carrigreen"><path d="M6.993 12c0 2.761 2.246 5.007 5.007 5.007s5.007-2.246 5.007-5.007S14.761 6.993 12 6.993 6.993 9.239 6.993 12zM12 8.993c1.658 0 3.007 1.349 3.007 3.007S13.658 15.007 12 15.007 8.993 13.658 8.993 12 10.342 8.993 12 8.993zM10.998 19h2v3h-2zm0-17h2v3h-2zm-9 9h3v2h-3zm17 0h3v2h-3zM4.219 18.363l2.12-2.122 1.415 1.414-2.12 2.122zM16.24 6.344l2.122-2.122 1.414 1.414-2.122 2.122zM6.342 7.759 4.22 5.637l1.415-1.414 2.12 2.122zm13.434 10.605-1.414 1.414-2.122-2.122 1.414-1.414z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" class="dark:hidden transform scale-90 md:hover:fill-marrsgreen "><path d="M20.742 13.045a8.088 8.088 0 0 1-2.077.271c-2.135 0-4.14-.83-5.646-2.336a8.025 8.025 0 0 1-2.064-7.723A1 1 0 0 0 9.73 2.034a10.014 10.014 0 0 0-4.489 2.582c-3.898 3.898-3.898 10.243 0 14.143a9.937 9.937 0 0 0 7.072 2.93 9.93 9.93 0 0 0 7.07-2.929 10.007 10.007 0 0 0 2.583-4.491 1.001 1.001 0 0 0-1.224-1.224zm-2.772 4.301a7.947 7.947 0 0 1-5.656 2.343 7.953 7.953 0 0 1-5.658-2.344c-3.118-3.119-3.118-8.195 0-11.314a7.923 7.923 0 0 1 2.06-1.483 10.027 10.027 0 0 0 2.89 7.848 9.972 9.972 0 0 0 7.848 2.891 8.036 8.036 0 0 1-1.484 2.059z"></path></svg></button></nav></div></div></header><div class="hidden fixed left-10 bottom-0 md:flex flex-col w-6 h-56 items-center justify-between"><div class="-rotate-90 text-lg tracking-widest"><a href="mailto:undefined" class="link-outline hover:text-marrsgreen dark:hover:text-carrigreen"></a></div><div class="w-40 h-1 bg-bgdark dark:bg-bglight rotate-90"></div></div><div class="hidden fixed right-10 bottom-0 md:flex flex-col w-6 h-[17rem] items-center justify-between"><div class="flex flex-col space-y-6"><a title="Arjun Subbiah&#x27;s Github Profile" href="https://github.com/arjunsubbiah" class="scale-110 rounded link-outline"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" class="dark:fill-bglight hover:fill-marrsgreen dark:hover:fill-carrigreen"><path fill-rule="evenodd" clip-rule="evenodd" d="M12.026 2c-5.509 0-9.974 4.465-9.974 9.974 0 4.406 2.857 8.145 6.821 9.465.499.09.679-.217.679-.481 0-.237-.008-.865-.011-1.696-2.775.602-3.361-1.338-3.361-1.338-.452-1.152-1.107-1.459-1.107-1.459-.905-.619.069-.605.069-.605 1.002.07 1.527 1.028 1.527 1.028.89 1.524 2.336 1.084 2.902.829.091-.645.351-1.085.635-1.334-2.214-.251-4.542-1.107-4.542-4.93 0-1.087.389-1.979 1.024-2.675-.101-.253-.446-1.268.099-2.64 0 0 .837-.269 2.742 1.021a9.582 9.582 0 0 1 2.496-.336 9.554 9.554 0 0 1 2.496.336c1.906-1.291 2.742-1.021 2.742-1.021.545 1.372.203 2.387.099 2.64.64.696 1.024 1.587 1.024 2.675 0 3.833-2.33 4.675-4.552 4.922.355.308.675.916.675 1.846 0 1.334-.012 2.41-.012 2.737 0 .267.178.577.687.479C19.146 20.115 22 16.379 22 11.974 22 6.465 17.535 2 12.026 2z"></path></svg></a><a title="Arjun Subbiah&#x27;s LinkedIn Profile" href="https://www.linkedin.com/in/arjun-subbiah-b19330a6/" class="scale-110 rounded link-outline"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" class="dark:fill-bglight hover:fill-marrsgreen dark:hover:fill-carrigreen"><path d="M20 3H4a1 1 0 0 0-1 1v16a1 1 0 0 0 1 1h16a1 1 0 0 0 1-1V4a1 1 0 0 0-1-1zM8.339 18.337H5.667v-8.59h2.672v8.59zM7.003 8.574a1.548 1.548 0 1 1 0-3.096 1.548 1.548 0 0 1 0 3.096zm11.335 9.763h-2.669V14.16c0-.996-.018-2.277-1.388-2.277-1.39 0-1.601 1.086-1.601 2.207v4.248h-2.667v-8.59h2.56v1.174h.037c.355-.675 1.227-1.387 2.524-1.387 2.704 0 3.203 1.778 3.203 4.092v4.71z"></path></svg></a><a title="Check Arjun Subbiah on Dev.to" href="https://dev.to/arjunsubbiah" class="scale-110 rounded link-outline"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" class="dark:fill-bglight hover:fill-marrsgreen dark:hover:fill-carrigreen"><path d="M7.826 10.083a.784.784 0 0 0-.468-.175h-.701v4.198h.701a.786.786 0 0 0 .469-.175c.155-.117.233-.292.233-.525v-2.798c.001-.233-.079-.408-.234-.525zM19.236 3H4.764C3.791 3 3.002 3.787 3 4.76v14.48c.002.973.791 1.76 1.764 1.76h14.473c.973 0 1.762-.787 1.764-1.76V4.76A1.765 1.765 0 0 0 19.236 3zM9.195 13.414c0 .755-.466 1.901-1.942 1.898H5.389V8.665h1.903c1.424 0 1.902 1.144 1.903 1.899v2.85zm4.045-3.562H11.1v1.544h1.309v1.188H11.1v1.543h2.142v1.188h-2.498a.813.813 0 0 1-.833-.792V9.497a.813.813 0 0 1 .792-.832h2.539l-.002 1.187zm4.165 4.632c-.531 1.235-1.481.99-1.906 0l-1.548-5.818h1.309l1.193 4.569 1.188-4.569h1.31l-1.546 5.818z"></path></svg></a><a title="Arjun Subbiah&#x27;s Profile on Facebook" href="https://www.facebook.com/arjunsubbiah" class="scale-110 rounded link-outline"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" class="dark:fill-bglight hover:fill-marrsgreen dark:hover:fill-carrigreen"><path d="M12.001 2.002c-5.522 0-9.999 4.477-9.999 9.999 0 4.99 3.656 9.126 8.437 9.879v-6.988h-2.54v-2.891h2.54V9.798c0-2.508 1.493-3.891 3.776-3.891 1.094 0 2.24.195 2.24.195v2.459h-1.264c-1.24 0-1.628.772-1.628 1.563v1.875h2.771l-.443 2.891h-2.328v6.988C18.344 21.129 22 16.992 22 12.001c0-5.522-4.477-9.999-9.999-9.999z"></path></svg></a></div><div class="w-40 h-1 bg-bgdark dark:bg-bglight rotate-90"></div></div><main id="main" class="blog-main"><article class="blog-section"><h1 class="font-semibold md:font-bold text-3xl md:text-4xl">Decision Trees: From Basics to Random Forests</h1><div class="mt-2 mb-1 italic text-marrsdark dark:text-carrigreen"><div class="relative"><span class="sr-only">Posted on: </span> <span aria-hidden="true">|</span><span class="sr-only"> at </span> </div></div><span class="flex items-center mt-2 mb-2 px-2 border-l-4 border-marrsgreen dark:border-carrigreen"><svg xmlns="http://www.w3.org/2000/svg" class="h-6 w-6 inline-block mr-2" fill="none" viewBox="0 0 24 24" aria-label="Category" stroke="currentColor" stroke-width="1"><path stroke-linecap="round" stroke-linejoin="round" d="M5 19a2 2 0 01-2-2V7a2 2 0 012-2h4l2 2h4a2 2 0 012 2v1M5 19h14a2 2 0 002-2v-5a2 2 0 00-2-2H9a2 2 0 00-2 2v5a2 2 0 01-2 2z"></path></svg> <a class="hover:text-marrsgreen dark:hover:text-carrigreen font-medium outline-marrsgreen dark:outline-carrigreen" href="/blog/categories/machine-learning">Machine Learning</a></span><div class="my-2"><a class="blog-tag link-outline group whitespace-nowrap dark:fill-bglight hover:bg-marrsgreen hover:text-bglight hover:fill-bglight dark:hover:bg-carrigreen dark:hover:text-bgdark dark:hover:fill-bgdark py-1 px-2 text-xs mr-2 my-1 bg-gray-300 dark:bg-carddark rounded inline-block shadow hover:shadow-md cursor-pointer" href="/blog/tags/machine-learning"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" class="inline-block scale-75 mr-1"><path d="M13.707 3.293A.996.996 0 0 0 13 3H4a1 1 0 0 0-1 1v9c0 .266.105.52.293.707l8 8a.997.997 0 0 0 1.414 0l9-9a.999.999 0 0 0 0-1.414l-8-8zM12 19.586l-7-7V5h7.586l7 7L12 19.586z"></path><circle cx="8.496" cy="8.495" r="1.505"></circle></svg>machine-learning</a><a class="blog-tag link-outline group whitespace-nowrap dark:fill-bglight hover:bg-marrsgreen hover:text-bglight hover:fill-bglight dark:hover:bg-carrigreen dark:hover:text-bgdark dark:hover:fill-bgdark py-1 px-2 text-xs mr-2 my-1 bg-gray-300 dark:bg-carddark rounded inline-block shadow hover:shadow-md cursor-pointer" href="/blog/tags/decision-trees"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" class="inline-block scale-75 mr-1"><path d="M13.707 3.293A.996.996 0 0 0 13 3H4a1 1 0 0 0-1 1v9c0 .266.105.52.293.707l8 8a.997.997 0 0 0 1.414 0l9-9a.999.999 0 0 0 0-1.414l-8-8zM12 19.586l-7-7V5h7.586l7 7L12 19.586z"></path><circle cx="8.496" cy="8.495" r="1.505"></circle></svg>decision-trees</a><a class="blog-tag link-outline group whitespace-nowrap dark:fill-bglight hover:bg-marrsgreen hover:text-bglight hover:fill-bglight dark:hover:bg-carrigreen dark:hover:text-bgdark dark:hover:fill-bgdark py-1 px-2 text-xs mr-2 my-1 bg-gray-300 dark:bg-carddark rounded inline-block shadow hover:shadow-md cursor-pointer" href="/blog/tags/random-forest"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" class="inline-block scale-75 mr-1"><path d="M13.707 3.293A.996.996 0 0 0 13 3H4a1 1 0 0 0-1 1v9c0 .266.105.52.293.707l8 8a.997.997 0 0 0 1.414 0l9-9a.999.999 0 0 0 0-1.414l-8-8zM12 19.586l-7-7V5h7.586l7 7L12 19.586z"></path><circle cx="8.496" cy="8.495" r="1.505"></circle></svg>random-forest</a><a class="blog-tag link-outline group whitespace-nowrap dark:fill-bglight hover:bg-marrsgreen hover:text-bglight hover:fill-bglight dark:hover:bg-carrigreen dark:hover:text-bgdark dark:hover:fill-bgdark py-1 px-2 text-xs mr-2 my-1 bg-gray-300 dark:bg-carddark rounded inline-block shadow hover:shadow-md cursor-pointer" href="/blog/tags/ensemble-methods"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" class="inline-block scale-75 mr-1"><path d="M13.707 3.293A.996.996 0 0 0 13 3H4a1 1 0 0 0-1 1v9c0 .266.105.52.293.707l8 8a.997.997 0 0 0 1.414 0l9-9a.999.999 0 0 0 0-1.414l-8-8zM12 19.586l-7-7V5h7.586l7 7L12 19.586z"></path><circle cx="8.496" cy="8.495" r="1.505"></circle></svg>ensemble-methods</a></div><div class="prose prose-img:mx-auto prose-lg prose-headings:font-medium prose-h2:mb-3 prose-h3:mb-1 prose-h3:mt-6 prose-h3:text-xl prose-h3:font-bold prose-h3:italic prose-h3:text-marrsgreen dark:prose-h3:text-carrigreen prose-p:mt-1 prose-p:my-3 mx-auto max-w-4xl dark:prose-invert"><div><h1>Decision Trees: From Basics to Random Forests</h1>
<p>Decision trees are among the most intuitive machine learning algorithms. Having used decision tree-based solutions that saved millions in operational costs, I want to share practical insights on when and how to use them effectively.</p>
<h2>What Are Decision Trees?</h2>
<p>Decision trees create predictive models by learning simple decision rules from data features. Think of them as a series of yes/no questions that lead to a prediction.</p>
<h2>How Decision Trees Work</h2>
<p>The algorithm builds a tree by:</p>
<ol>
<li><strong>Selecting the Best Split</strong>: Choose the feature and threshold that best separates the data</li>
<li><strong>Recursive Partitioning</strong>: Apply the same process to each subset</li>
<li><strong>Stopping</strong>: When criteria are met (max depth, min samples, etc.)</li>
<li><strong>Prediction</strong>: Follow the path from root to leaf</li>
</ol>
<h2>Mathematical Foundation</h2>
<h3>Impurity Measures</h3>
<p>Decision trees use impurity measures to find the best splits:</p>
<p><strong>Gini Impurity</strong> (Classification):
$$Gini(t) = 1 - \sum_{i} [p(i|t)]^2$$</p>
<p><strong>Entropy</strong> (Classification):
$$Entropy(t) = -\sum_{i} p(i|t) \log_2 p(i|t)$$</p>
<p><strong>Mean Squared Error</strong> (Regression):
$$MSE(t) = \frac{1}{n} \sum_{i} (y_i - \bar{y})^2$$</p>
<h3>Information Gain</h3>
<p>Information gain measures the reduction in impurity:
$$IG = Impurity(parent) - \sum_{j} \left( \frac{n_j}{n} \times Impurity(child_j) \right)$$</p>
<h2>Complete Implementation</h2>
<pre><code>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.datasets import make_classification, make_regression
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score, mean_squared_error, classification_report
from sklearn.tree import plot_tree
import seaborn as sns

# Generate sample classification data
X_class, y_class = make_classification(
    n_samples=1000, 
    n_features=4, 
    n_informative=3, 
    n_redundant=1, 
    n_clusters_per_class=1, 
    random_state=42
)

# Generate sample regression data
X_reg, y_reg = make_regression(
    n_samples=1000, 
    n_features=4, 
    noise=0.1, 
    random_state=42
)

# Split data
X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(
    X_class, y_class, test_size=0.2, random_state=42
)

X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(
    X_reg, y_reg, test_size=0.2, random_state=42
)
</code></pre>
<h2>Decision Tree Implementation</h2>
<h3>Classification Example</h3>
<pre><code># Create and train decision tree classifier
dt_classifier = DecisionTreeClassifier(
    max_depth=5,
    min_samples_split=20,
    min_samples_leaf=10,
    random_state=42
)

dt_classifier.fit(X_train_c, y_train_c)

# Make predictions
y_pred_dt = dt_classifier.predict(X_test_c)
dt_accuracy = accuracy_score(y_test_c, y_pred_dt)

print("Decision Tree Classifier Results:")
print(f"Accuracy: {dt_accuracy:.4f}")
print("\nDetailed Classification Report:")
print(classification_report(y_test_c, y_pred_dt))

# Feature importance
feature_importance = dt_classifier.feature_importances_
print(f"\nFeature Importances: {feature_importance}")
</code></pre>
<h3>Regression Example</h3>
<pre><code># Decision tree regressor
dt_regressor = DecisionTreeRegressor(
    max_depth=5,
    min_samples_split=20,
    min_samples_leaf=10,
    random_state=42
)

dt_regressor.fit(X_train_r, y_train_r)

# Predictions and evaluation
y_pred_dt_reg = dt_regressor.predict(X_test_r)
dt_mse = mean_squared_error(y_test_r, y_pred_dt_reg)
dt_rmse = np.sqrt(dt_mse)

print(f"Decision Tree Regressor RMSE: {dt_rmse:.4f}")
</code></pre>
<h2>Hyperparameter Tuning</h2>
<p>Key parameters to tune:</p>
<pre><code>from sklearn.model_selection import GridSearchCV

# Define parameter grid
param_grid = {
    'max_depth': [3, 5, 7, 10, None],
    'min_samples_split': [2, 10, 20],
    'min_samples_leaf': [1, 5, 10],
    'max_features': ['sqrt', 'log2', None]
}

# Grid search
grid_search = GridSearchCV(
    DecisionTreeClassifier(random_state=42),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)

grid_search.fit(X_train_c, y_train_c)

print("Best Parameters:", grid_search.best_params_)
print("Best Cross-validation Score:", grid_search.best_score_)

# Use best model
best_dt = grid_search.best_estimator_
</code></pre>
<h2>Tree Visualization</h2>
<pre><code># Visualize the decision tree
plt.figure(figsize=(20, 10))
plot_tree(
    dt_classifier,
    max_depth=3,  # Limit depth for readability
    feature_names=[f'Feature_{i}' for i in range(X_class.shape[1])],
    class_names=['Class_0', 'Class_1'],
    filled=True,
    rounded=True,
    fontsize=10
)
plt.title("Decision Tree Visualization (Max Depth 3)")
plt.show()
</code></pre>
<h2>Random Forest: Ensemble Power</h2>
<p>Decision trees are prone to overfitting. Random Forests overcome this by building multiple decision trees and aggregating their predictions.</p>
<h3>How Random Forests Work</h3>
<ol>
<li><strong>Bagging</strong>: Each tree is built on a bootstrap sample of the data.</li>
<li><strong>Feature Randomness</strong>: At each split, only a random subset of features is considered.</li>
<li><strong>Aggregation</strong>:
<ul>
<li>Classification: Majority vote</li>
<li>Regression: Average of predictions</li>
</ul>
</li>
</ol>
<h2>Random Forest Implementation</h2>
<pre><code># Random Forest Classifier
rf_classifier = RandomForestClassifier(
    n_estimators=100,  # Number of trees
    max_depth=10,
    min_samples_split=10,
    min_samples_leaf=5,
    random_state=42,
    n_jobs=-1  # Use all available cores
)

rf_classifier.fit(X_train_c, y_train_c)
y_pred_rf = rf_classifier.predict(X_test_c)
rf_accuracy = accuracy_score(y_test_c, y_pred_rf)

print("\nRandom Forest Classifier Results:")
print(f"Accuracy: {rf_accuracy:.4f}")
print("\nDetailed Classification Report (Random Forest):")
print(classification_report(y_test_c, y_pred_rf))

# Random Forest Regressor
rf_regressor = RandomForestRegressor(
    n_estimators=100,
    max_depth=10,
    min_samples_split=10,
    min_samples_leaf=5,
    random_state=42,
    n_jobs=-1
)

rf_regressor.fit(X_train_r, y_train_r)
y_pred_rf_reg = rf_regressor.predict(X_test_r)
rf_mse = mean_squared_error(y_test_r, y_pred_rf_reg)
rf_rmse = np.sqrt(rf_mse)

print(f"Random Forest Regressor RMSE: {rf_rmse:.4f}")

# Feature importance from Random Forest
rf_feature_importance = rf_classifier.feature_importances_
print(f"\nRandom Forest Feature Importances: {rf_feature_importance}")
</code></pre>
<h2>Advantages of Decision Trees and Random Forests</h2>
<h3>Decision Trees:</h3>
<ul>
<li><strong>Interpretability</strong>: Easy to understand and visualize (especially small trees).</li>
<li><strong>No Scaling Needed</strong>: Not sensitive to feature scaling.</li>
<li><strong>Handle Mixed Data</strong>: Can handle both numerical and categorical data.</li>
</ul>
<h3>Random Forests:</h3>
<ul>
<li><strong>High Accuracy</strong>: Generally perform very well.</li>
<li><strong>Robust to Overfitting</strong>: Due to bagging and feature randomness.</li>
<li><strong>Feature Importance</strong>: Provide a good measure of feature importance.</li>
<li><strong>Handle Missing Values</strong>: Can handle missing values implicitly.</li>
</ul>
<h2>Disadvantages</h2>
<h3>Decision Trees:</h3>
<ul>
<li><strong>Overfitting</strong>: Prone to overfitting if not pruned or depth-limited.</li>
<li><strong>Instability</strong>: Small changes in data can lead to a very different tree.</li>
</ul>
<h3>Random Forests:</h3>
<ul>
<li><strong>Less Interpretable</strong>: More of a "black box" compared to single trees.</li>
<li><strong>Computational Cost</strong>: More trees mean longer training times.</li>
</ul>
<h2>Real-World Applications</h2>
<p>As a Staff Data Scientist, I've leveraged these algorithms for critical business problems:</p>
<h3>1. Fraud Detection</h3>
<p>Used Random Forests to identify anomalous transactions and fraudulent driver behavior, significantly reducing financial losses.</p>
<pre><code># Example for fraud detection
# features = ['transaction_amount', 'location_deviation', 'time_of_day', 'user_history_score']
# fraud_model = RandomForestClassifier()
</code></pre>
<h3>2. Customer Churn Prediction</h3>
<p>Decision trees helped understand key drivers of customer churn by identifying decision rules (e.g., "if customer calls support > 3 times and plan cost > X, then churn risk is high").</p>
<h3>3. Medical Diagnosis</h3>
<p>Random Forests can be used for classifying diseases based on patient symptoms and test results.</p>
<h3>4. Supply Chain Optimization</h3>
<p>Used decision tree-based models to predict equipment failures, optimizing maintenance schedules and reducing downtime.</p>
<h2>When to Use Which?</h2>
<p><strong>Decision Tree</strong>: When interpretability is paramount, for quick baselines, or for rule extraction.</p>
<p><strong>Random Forest</strong>: When high predictive accuracy is needed, robust performance against overfitting, and dealing with complex, high-dimensional data.</p>
<h2>Production Best Practices</h2>
<p>Deploying tree-based models requires careful attention:</p>
<h3>1. Model Monitoring</h3>
<p>Monitor feature drift and model performance. Tree models can be sensitive to shifts in data distributions.</p>
<h3>2. Explainability (XAI)</h3>
<p>For Random Forests, use SHAP or LIME to provide local interpretability, even though the overall model is a black box.</p>
<h3>3. Cross-Validation</h3>
<p>Always use robust cross-validation (e.g., K-fold) to get reliable performance estimates and tune hyperparameters.</p>
<h3>4. Ensemble More Than Trees</h3>
<p>Consider Gradient Boosting Machines (like XGBoost, LightGBM) for even higher performance, especially if you have highly correlated features or need to capture complex interactions.</p>
<h2>Conclusion</h2>
<p>Decision Trees and Random Forests are fundamental tools in a data scientist's toolkit. While simple decision trees offer unparalleled interpretability, Random Forests elevate their power by combining multiple trees into a robust, high-performing ensemble.</p>
<p>Mastering these algorithms, from their theoretical underpinnings to practical production deployment, is key to delivering impactful machine learning solutions.</p>
<p>Do you prefer the simplicity of a single decision tree or the power of a Random Forest? Share your experiences and use cases in the comments!</p>
</div></div><div class="fb-share-button my-4" data-href="undefined/blog/posts/decision-trees-random-forests" data-layout="button" data-size="large"><a rel="noopener noreferrer" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=undefined%2Fblog%2Fposts%2Fdecision-trees-random-forests&amp;amp;src=sdkpreparse" class="fb-xfbml-parse-ignore">Share</a></div></article></main><footer class="pb-24 md:pb-4 text-center mt-auto"><div class="flex justify-center space-x-12 mb-4"><a href="https://github.com/arjunsubbiah" title="Arjun Chidambaram Subbiah&#x27;s Github Profile" class="transform scale-150 md:scale-125 link-outline"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" class="dark:fill-bglight hover:fill-marrsgreen dark:hover:fill-carrigreen"><path fill-rule="evenodd" clip-rule="evenodd" d="M12.026 2c-5.509 0-9.974 4.465-9.974 9.974 0 4.406 2.857 8.145 6.821 9.465.499.09.679-.217.679-.481 0-.237-.008-.865-.011-1.696-2.775.602-3.361-1.338-3.361-1.338-.452-1.152-1.107-1.459-1.107-1.459-.905-.619.069-.605.069-.605 1.002.07 1.527 1.028 1.527 1.028.89 1.524 2.336 1.084 2.902.829.091-.645.351-1.085.635-1.334-2.214-.251-4.542-1.107-4.542-4.93 0-1.087.389-1.979 1.024-2.675-.101-.253-.446-1.268.099-2.64 0 0 .837-.269 2.742 1.021a9.582 9.582 0 0 1 2.496-.336 9.554 9.554 0 0 1 2.496.336c1.906-1.291 2.742-1.021 2.742-1.021.545 1.372.203 2.387.099 2.64.64.696 1.024 1.587 1.024 2.675 0 3.833-2.33 4.675-4.552 4.922.355.308.675.916.675 1.846 0 1.334-.012 2.41-.012 2.737 0 .267.178.577.687.479C19.146 20.115 22 16.379 22 11.974 22 6.465 17.535 2 12.026 2z"></path></svg></a><a href="https://linkedin.com/in/arjun-subbiah-b19330a6/" title="Arjun Chidambaram Subbiah&#x27;s LinkedIn Profile" class="transform scale-150 md:scale-125 link-outline"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" class="dark:fill-bglight hover:fill-marrsgreen dark:hover:fill-carrigreen"><path d="M20 3H4a1 1 0 0 0-1 1v16a1 1 0 0 0 1 1h16a1 1 0 0 0 1-1V4a1 1 0 0 0-1-1zM8.339 18.337H5.667v-8.59h2.672v8.59zM7.003 8.574a1.548 1.548 0 1 1 0-3.096 1.548 1.548 0 0 1 0 3.096zm11.335 9.763h-2.669V14.16c0-.996-.018-2.277-1.388-2.277-1.39 0-1.601 1.086-1.601 2.207v4.248h-2.667v-8.59h2.56v1.174h.037c.355-.675 1.227-1.387 2.524-1.387 2.704 0 3.203 1.778 3.203 4.092v4.71z"></path></svg></a><a href="https://twitter.com/arjunsubbiah" title="Follow Arjun Chidambaram Subbiah on Twitter" class="transform scale-150 md:scale-125 link-outline"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" class="dark:fill-bglight hover:fill-marrsgreen dark:hover:fill-carrigreen"><path d="M19.633 7.997c.013.175.013.349.013.523 0 5.325-4.053 11.461-11.46 11.461-2.282 0-4.402-.661-6.186-1.809.324.037.636.05.973.05a8.07 8.07 0 0 0 5.001-1.721 4.036 4.036 0 0 1-3.767-2.793c.249.037.499.062.761.062.361 0 .724-.05 1.061-.137a4.027 4.027 0 0 1-3.23-3.953v-.05c.537.299 1.16.486 1.82.511a4.022 4.022 0 0 1-1.796-3.354c0-.748.199-1.434.548-2.032a11.457 11.457 0 0 0 8.306 4.215c-.062-.3-.1-.611-.1-.923a4.026 4.026 0 0 1 4.028-4.028c1.16 0 2.207.486 2.943 1.272a7.957 7.957 0 0 0 2.556-.973 4.02 4.02 0 0 1-1.771 2.22 8.073 8.073 0 0 0 2.319-.624 8.645 8.645 0 0 1-2.019 2.083z"></path></svg></a></div><div>Coded with <span class="sr-only">love</span><svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mx-1 inline-block mb-1" fill="none" viewBox="0 0 24 24" stroke="currentColor" aria-hidden="true" stroke-width="2"><path stroke-linecap="round" stroke-linejoin="round" d="M4.318 6.318a4.5 4.5 0 000 6.364L12 20.364l7.682-7.682a4.5 4.5 0 00-6.364-6.364L12 7.636l-1.318-1.318a4.5 4.5 0 00-6.364 0z"></path></svg> <!-- -->by <!-- -->Arjun Chidambaram Subbiah</div></footer></div></div></div><script id="__NEXT_DATA__" type="application/json" crossorigin="">{"props":{"pageProps":{"post":{"title":"Decision Trees: From Basics to Random Forests","datetime":"2024-07-19T10:35:07.000Z","description":"Master decision trees and understand how they evolve into powerful ensemble methods like Random Forests.","slug":"decision-trees-random-forests","author":"Arjun Subbiah","content":"\u003ch1\u003eDecision Trees: From Basics to Random Forests\u003c/h1\u003e\n\u003cp\u003eDecision trees are among the most intuitive machine learning algorithms. Having used decision tree-based solutions that saved millions in operational costs, I want to share practical insights on when and how to use them effectively.\u003c/p\u003e\n\u003ch2\u003eWhat Are Decision Trees?\u003c/h2\u003e\n\u003cp\u003eDecision trees create predictive models by learning simple decision rules from data features. Think of them as a series of yes/no questions that lead to a prediction.\u003c/p\u003e\n\u003ch2\u003eHow Decision Trees Work\u003c/h2\u003e\n\u003cp\u003eThe algorithm builds a tree by:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eSelecting the Best Split\u003c/strong\u003e: Choose the feature and threshold that best separates the data\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRecursive Partitioning\u003c/strong\u003e: Apply the same process to each subset\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eStopping\u003c/strong\u003e: When criteria are met (max depth, min samples, etc.)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePrediction\u003c/strong\u003e: Follow the path from root to leaf\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003eMathematical Foundation\u003c/h2\u003e\n\u003ch3\u003eImpurity Measures\u003c/h3\u003e\n\u003cp\u003eDecision trees use impurity measures to find the best splits:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eGini Impurity\u003c/strong\u003e (Classification):\n$$Gini(t) = 1 - \\sum_{i} [p(i|t)]^2$$\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eEntropy\u003c/strong\u003e (Classification):\n$$Entropy(t) = -\\sum_{i} p(i|t) \\log_2 p(i|t)$$\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eMean Squared Error\u003c/strong\u003e (Regression):\n$$MSE(t) = \\frac{1}{n} \\sum_{i} (y_i - \\bar{y})^2$$\u003c/p\u003e\n\u003ch3\u003eInformation Gain\u003c/h3\u003e\n\u003cp\u003eInformation gain measures the reduction in impurity:\n$$IG = Impurity(parent) - \\sum_{j} \\left( \\frac{n_j}{n} \\times Impurity(child_j) \\right)$$\u003c/p\u003e\n\u003ch2\u003eComplete Implementation\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003eimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.datasets import make_classification, make_regression\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import accuracy_score, mean_squared_error, classification_report\nfrom sklearn.tree import plot_tree\nimport seaborn as sns\n\n# Generate sample classification data\nX_class, y_class = make_classification(\n    n_samples=1000, \n    n_features=4, \n    n_informative=3, \n    n_redundant=1, \n    n_clusters_per_class=1, \n    random_state=42\n)\n\n# Generate sample regression data\nX_reg, y_reg = make_regression(\n    n_samples=1000, \n    n_features=4, \n    noise=0.1, \n    random_state=42\n)\n\n# Split data\nX_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n    X_class, y_class, test_size=0.2, random_state=42\n)\n\nX_train_r, X_test_r, y_train_r, y_test_r = train_test_split(\n    X_reg, y_reg, test_size=0.2, random_state=42\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eDecision Tree Implementation\u003c/h2\u003e\n\u003ch3\u003eClassification Example\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003e# Create and train decision tree classifier\ndt_classifier = DecisionTreeClassifier(\n    max_depth=5,\n    min_samples_split=20,\n    min_samples_leaf=10,\n    random_state=42\n)\n\ndt_classifier.fit(X_train_c, y_train_c)\n\n# Make predictions\ny_pred_dt = dt_classifier.predict(X_test_c)\ndt_accuracy = accuracy_score(y_test_c, y_pred_dt)\n\nprint(\"Decision Tree Classifier Results:\")\nprint(f\"Accuracy: {dt_accuracy:.4f}\")\nprint(\"\\nDetailed Classification Report:\")\nprint(classification_report(y_test_c, y_pred_dt))\n\n# Feature importance\nfeature_importance = dt_classifier.feature_importances_\nprint(f\"\\nFeature Importances: {feature_importance}\")\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eRegression Example\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003e# Decision tree regressor\ndt_regressor = DecisionTreeRegressor(\n    max_depth=5,\n    min_samples_split=20,\n    min_samples_leaf=10,\n    random_state=42\n)\n\ndt_regressor.fit(X_train_r, y_train_r)\n\n# Predictions and evaluation\ny_pred_dt_reg = dt_regressor.predict(X_test_r)\ndt_mse = mean_squared_error(y_test_r, y_pred_dt_reg)\ndt_rmse = np.sqrt(dt_mse)\n\nprint(f\"Decision Tree Regressor RMSE: {dt_rmse:.4f}\")\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eHyperparameter Tuning\u003c/h2\u003e\n\u003cp\u003eKey parameters to tune:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom sklearn.model_selection import GridSearchCV\n\n# Define parameter grid\nparam_grid = {\n    'max_depth': [3, 5, 7, 10, None],\n    'min_samples_split': [2, 10, 20],\n    'min_samples_leaf': [1, 5, 10],\n    'max_features': ['sqrt', 'log2', None]\n}\n\n# Grid search\ngrid_search = GridSearchCV(\n    DecisionTreeClassifier(random_state=42),\n    param_grid,\n    cv=5,\n    scoring='accuracy',\n    n_jobs=-1\n)\n\ngrid_search.fit(X_train_c, y_train_c)\n\nprint(\"Best Parameters:\", grid_search.best_params_)\nprint(\"Best Cross-validation Score:\", grid_search.best_score_)\n\n# Use best model\nbest_dt = grid_search.best_estimator_\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eTree Visualization\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003e# Visualize the decision tree\nplt.figure(figsize=(20, 10))\nplot_tree(\n    dt_classifier,\n    max_depth=3,  # Limit depth for readability\n    feature_names=[f'Feature_{i}' for i in range(X_class.shape[1])],\n    class_names=['Class_0', 'Class_1'],\n    filled=True,\n    rounded=True,\n    fontsize=10\n)\nplt.title(\"Decision Tree Visualization (Max Depth 3)\")\nplt.show()\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eRandom Forest: Ensemble Power\u003c/h2\u003e\n\u003cp\u003eDecision trees are prone to overfitting. Random Forests overcome this by building multiple decision trees and aggregating their predictions.\u003c/p\u003e\n\u003ch3\u003eHow Random Forests Work\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eBagging\u003c/strong\u003e: Each tree is built on a bootstrap sample of the data.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eFeature Randomness\u003c/strong\u003e: At each split, only a random subset of features is considered.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAggregation\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003eClassification: Majority vote\u003c/li\u003e\n\u003cli\u003eRegression: Average of predictions\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003eRandom Forest Implementation\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003e# Random Forest Classifier\nrf_classifier = RandomForestClassifier(\n    n_estimators=100,  # Number of trees\n    max_depth=10,\n    min_samples_split=10,\n    min_samples_leaf=5,\n    random_state=42,\n    n_jobs=-1  # Use all available cores\n)\n\nrf_classifier.fit(X_train_c, y_train_c)\ny_pred_rf = rf_classifier.predict(X_test_c)\nrf_accuracy = accuracy_score(y_test_c, y_pred_rf)\n\nprint(\"\\nRandom Forest Classifier Results:\")\nprint(f\"Accuracy: {rf_accuracy:.4f}\")\nprint(\"\\nDetailed Classification Report (Random Forest):\")\nprint(classification_report(y_test_c, y_pred_rf))\n\n# Random Forest Regressor\nrf_regressor = RandomForestRegressor(\n    n_estimators=100,\n    max_depth=10,\n    min_samples_split=10,\n    min_samples_leaf=5,\n    random_state=42,\n    n_jobs=-1\n)\n\nrf_regressor.fit(X_train_r, y_train_r)\ny_pred_rf_reg = rf_regressor.predict(X_test_r)\nrf_mse = mean_squared_error(y_test_r, y_pred_rf_reg)\nrf_rmse = np.sqrt(rf_mse)\n\nprint(f\"Random Forest Regressor RMSE: {rf_rmse:.4f}\")\n\n# Feature importance from Random Forest\nrf_feature_importance = rf_classifier.feature_importances_\nprint(f\"\\nRandom Forest Feature Importances: {rf_feature_importance}\")\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eAdvantages of Decision Trees and Random Forests\u003c/h2\u003e\n\u003ch3\u003eDecision Trees:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eInterpretability\u003c/strong\u003e: Easy to understand and visualize (especially small trees).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eNo Scaling Needed\u003c/strong\u003e: Not sensitive to feature scaling.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eHandle Mixed Data\u003c/strong\u003e: Can handle both numerical and categorical data.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eRandom Forests:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eHigh Accuracy\u003c/strong\u003e: Generally perform very well.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRobust to Overfitting\u003c/strong\u003e: Due to bagging and feature randomness.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eFeature Importance\u003c/strong\u003e: Provide a good measure of feature importance.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eHandle Missing Values\u003c/strong\u003e: Can handle missing values implicitly.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eDisadvantages\u003c/h2\u003e\n\u003ch3\u003eDecision Trees:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eOverfitting\u003c/strong\u003e: Prone to overfitting if not pruned or depth-limited.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eInstability\u003c/strong\u003e: Small changes in data can lead to a very different tree.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eRandom Forests:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eLess Interpretable\u003c/strong\u003e: More of a \"black box\" compared to single trees.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eComputational Cost\u003c/strong\u003e: More trees mean longer training times.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eReal-World Applications\u003c/h2\u003e\n\u003cp\u003eAs a Staff Data Scientist, I've leveraged these algorithms for critical business problems:\u003c/p\u003e\n\u003ch3\u003e1. Fraud Detection\u003c/h3\u003e\n\u003cp\u003eUsed Random Forests to identify anomalous transactions and fraudulent driver behavior, significantly reducing financial losses.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# Example for fraud detection\n# features = ['transaction_amount', 'location_deviation', 'time_of_day', 'user_history_score']\n# fraud_model = RandomForestClassifier()\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e2. Customer Churn Prediction\u003c/h3\u003e\n\u003cp\u003eDecision trees helped understand key drivers of customer churn by identifying decision rules (e.g., \"if customer calls support \u003e 3 times and plan cost \u003e X, then churn risk is high\").\u003c/p\u003e\n\u003ch3\u003e3. Medical Diagnosis\u003c/h3\u003e\n\u003cp\u003eRandom Forests can be used for classifying diseases based on patient symptoms and test results.\u003c/p\u003e\n\u003ch3\u003e4. Supply Chain Optimization\u003c/h3\u003e\n\u003cp\u003eUsed decision tree-based models to predict equipment failures, optimizing maintenance schedules and reducing downtime.\u003c/p\u003e\n\u003ch2\u003eWhen to Use Which?\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eDecision Tree\u003c/strong\u003e: When interpretability is paramount, for quick baselines, or for rule extraction.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eRandom Forest\u003c/strong\u003e: When high predictive accuracy is needed, robust performance against overfitting, and dealing with complex, high-dimensional data.\u003c/p\u003e\n\u003ch2\u003eProduction Best Practices\u003c/h2\u003e\n\u003cp\u003eDeploying tree-based models requires careful attention:\u003c/p\u003e\n\u003ch3\u003e1. Model Monitoring\u003c/h3\u003e\n\u003cp\u003eMonitor feature drift and model performance. Tree models can be sensitive to shifts in data distributions.\u003c/p\u003e\n\u003ch3\u003e2. Explainability (XAI)\u003c/h3\u003e\n\u003cp\u003eFor Random Forests, use SHAP or LIME to provide local interpretability, even though the overall model is a black box.\u003c/p\u003e\n\u003ch3\u003e3. Cross-Validation\u003c/h3\u003e\n\u003cp\u003eAlways use robust cross-validation (e.g., K-fold) to get reliable performance estimates and tune hyperparameters.\u003c/p\u003e\n\u003ch3\u003e4. Ensemble More Than Trees\u003c/h3\u003e\n\u003cp\u003eConsider Gradient Boosting Machines (like XGBoost, LightGBM) for even higher performance, especially if you have highly correlated features or need to capture complex interactions.\u003c/p\u003e\n\u003ch2\u003eConclusion\u003c/h2\u003e\n\u003cp\u003eDecision Trees and Random Forests are fundamental tools in a data scientist's toolkit. While simple decision trees offer unparalleled interpretability, Random Forests elevate their power by combining multiple trees into a robust, high-performing ensemble.\u003c/p\u003e\n\u003cp\u003eMastering these algorithms, from their theoretical underpinnings to practical production deployment, is key to delivering impactful machine learning solutions.\u003c/p\u003e\n\u003cp\u003eDo you prefer the simplicity of a single decision tree or the power of a Random Forest? Share your experiences and use cases in the comments!\u003c/p\u003e\n","category":"Machine Learning","tags":["machine-learning","decision-trees","random-forest","ensemble-methods"]}},"__N_SSG":true},"page":"/blog/posts/[slug]","query":{"slug":"decision-trees-random-forests"},"buildId":"Oo8n1X-_SUSxiqmo-5vbM","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>